{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and sampling main HMDA data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  99%|█████████▉| 142/143 [01:04<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic dataset statistics:\n",
      "Number of applications: 500,500\n",
      "Number of approved loans: 271,308\n",
      "Overall approval rate: 54.21%\n"
     ]
    }
   ],
   "source": [
    "# part 1\n",
    "\n",
    "def load_and_prepare_data(main_file: str, sample_size: int = 500000) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Load and prepare the HMDA dataset using chunked processing and sampling.\n",
    "    Returns processed dataframe and data statistics dictionary.\n",
    "    \"\"\"\n",
    "    # Define only the columns we need\n",
    "    needed_columns = [\n",
    "        'action_taken',\n",
    "        'loan_amount_000s',\n",
    "        'applicant_income_000s',\n",
    "        'applicant_race_name_1',\n",
    "        'applicant_ethnicity',\n",
    "        'applicant_sex',\n",
    "        'hud_median_family_income',\n",
    "        'denial_reason_1',\n",
    "        'state_code',\n",
    "        'county_code',\n",
    "        'minority_population',\n",
    "        'tract_to_msamd_income'\n",
    "    ]\n",
    "    \n",
    "    print(\"Loading and sampling main HMDA data...\")\n",
    "    # Initialize empty list to store chunks\n",
    "    chunks = []\n",
    "    chunk_size = 100000  # Adjust this based on your available memory\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    for chunk in tqdm(pd.read_csv(main_file, \n",
    "                                usecols=needed_columns, \n",
    "                                chunksize=chunk_size),\n",
    "                    total=143, \n",
    "                    desc=\"Processing chunks\"):\n",
    "\n",
    "        # Sample from each chunk proportionally\n",
    "        chunk_sample_size = int(sample_size * (chunk_size / 14285496))  # Adjust for total record count\n",
    "        sampled_chunk = chunk.sample(n=min(chunk_sample_size, len(chunk)))\n",
    "        chunks.append(sampled_chunk)\n",
    "\n",
    "        # Print progress every 10 chunks\n",
    "        # if count % 10 == 0:\n",
    "            # print(f\"Processed chunk, current sample size: {sum(len(chunk) for chunk in chunks)}\")\n",
    "\n",
    "        # Break if we've reached the desired sample size\n",
    "        if sum(len(chunk) for chunk in chunks) >= sample_size:\n",
    "            break\n",
    "    \n",
    "    # Combine chunks\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Basic data cleaning\n",
    "    # print(\"Performing initial data cleaning...\")\n",
    "    \n",
    "    # Convert action_taken to binary (approved = 1, denied = 0)\n",
    "    df['approved'] = df['action_taken'].isin([1, 2]).astype(int)\n",
    "    \n",
    "    # Calculate approval rates by different demographics\n",
    "    stats = {}\n",
    "    \n",
    "    # Overall approval rate\n",
    "    stats['overall_approval_rate'] = df['approved'].mean()\n",
    "    \n",
    "    # Approval rates by race (using primary race)\n",
    "    race_approvals = df.groupby('applicant_race_name_1')['approved'].agg(['mean', 'count'])\n",
    "    stats['race_approval_rates'] = race_approvals\n",
    "    \n",
    "    # Approval rates by income bracket\n",
    "    df['income_bracket'] = pd.qcut(df['applicant_income_000s'].fillna(-1), \n",
    "                                 q=5, \n",
    "                                 labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "    income_approvals = df.groupby('income_bracket')['approved'].agg(['mean', 'count'])\n",
    "    stats['income_approval_rates'] = income_approvals\n",
    "    \n",
    "    # Calculate loan amount approval rates\n",
    "    # Create loan amount bins\n",
    "    df['loan_bin'] = pd.qcut(df['loan_amount_000s'].dropna(), q=10, labels=False)\n",
    "    loan_approval_rates = df.groupby('loan_bin')['approved'].mean()\n",
    "    stats['loan_amount_approval_rates'] = loan_approval_rates.tolist()\n",
    "    \n",
    "    print(\"\\nBasic dataset statistics:\")\n",
    "    print(f\"Number of applications: {len(df):,}\")\n",
    "    print(f\"Number of approved loans: {df['approved'].sum():,}\")\n",
    "    print(f\"Overall approval rate: {df['approved'].mean():.2%}\")\n",
    "    \n",
    "    return df, stats\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    main_file = 'HDMA/hmda_2017_nationwide_all-records_labels.csv'\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df, stats = load_and_prepare_data(main_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([425169, 9])\n"
     ]
    }
   ],
   "source": [
    "class MortgageDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, features: List[str], label_col: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): Processed DataFrame\n",
    "            features (List[str]): List of feature columns\n",
    "            label_col (str): Name of the target column\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(df[features].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df[label_col].values, dtype=torch.float32)\n",
    "\n",
    "        print(self.features.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "# Preprocessing function\n",
    "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Preprocess the HMDA dataset and create feature groups for analysis.\n",
    "    Returns preprocessed DataFrame and feature groups.\n",
    "    \"\"\"\n",
    "    # print(\"Starting data preprocessing...\")\n",
    "\n",
    "    # Create a copy of the data\n",
    "    data = df.copy()\n",
    "\n",
    "    # Define feature groups\n",
    "    feature_groups = {\n",
    "        'baseline': [\n",
    "            'loan_amount_000s',\n",
    "            'applicant_income_000s',\n",
    "            'hud_median_family_income',\n",
    "            'tract_to_msamd_income',\n",
    "            'applicant_race_name_1',\n",
    "            # 'applicant_ethnicity'\n",
    "            # 'income_to_loan_ratio',  # Will be created\n",
    "            # 'area_income_ratio'      # Will be created\n",
    "        # ],\n",
    "        # 'location': [\n",
    "        #     'state_code',\n",
    "        #     'county_code',\n",
    "        #     'minority_population'\n",
    "        # ],\n",
    "        # 'sensitive': [\n",
    "        #     'applicant_race_name_1',\n",
    "        #     'applicant_ethnicity',\n",
    "        #     'applicant_sex'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    columns_to_keep = [\n",
    "        'loan_amount_000s',\n",
    "        'applicant_income_000s',\n",
    "        'hud_median_family_income',\n",
    "        'tract_to_msamd_income',\n",
    "        'applicant_race_name_1',\n",
    "        # 'applicant_ethnicity',\n",
    "        'approved'\n",
    "    ]   \n",
    "\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # remove entries that are missing entries\n",
    "    data = data.dropna()\n",
    "\n",
    "    data = pd.get_dummies(data, columns=['applicant_race_name_1'], prefix='race')\n",
    "\n",
    "    to_remove = ['race_Information not provided by applicant in mail, Internet, or telephone application', 'race_Not applicable']\n",
    "\n",
    "    data = data.drop(columns=to_remove)\n",
    "\n",
    "    # print(data.columns)\n",
    "\n",
    "    # data['loan_amount_000s'] = data['loan_amount_000s'] / 1000\n",
    "    # data['applicant_income_000s'] = data['applicant_income_000s'] / 1000\n",
    "    # data['hud_median_family_income'] = data['hud_median_family_income'] / 100000\n",
    "    # data['tract_to_msamd_income'] = data['tract_to_msamd_income'] / 1000\n",
    "\n",
    "    # normalize values \n",
    "    data['loan_amount_000s'] = data['loan_amount_000s'] / data['loan_amount_000s'].mean()\n",
    "    data['applicant_income_000s'] = data['applicant_income_000s'] / data['applicant_income_000s'].mean()\n",
    "    data['hud_median_family_income'] = data['hud_median_family_income'] / data['hud_median_family_income'].mean()\n",
    "    data['tract_to_msamd_income'] = data['tract_to_msamd_income'] / data['tract_to_msamd_income'].mean()\n",
    "\n",
    "    # print(data['loan_amount_000s'].mean())\n",
    "    # print(data['applicant_income_000s'].mean())\n",
    "    # print(data['hud_median_family_income'].mean())\n",
    "    # print(data['tract_to_msamd_income'].mean())\n",
    "    \n",
    "    feature_groups = {\n",
    "        'baseline': ['loan_amount_000s', 'applicant_income_000s', 'hud_median_family_income',\n",
    "       'tract_to_msamd_income',\n",
    "       'race_American Indian or Alaska Native', 'race_Asian',\n",
    "       'race_Black or African American',\n",
    "       'race_Native Hawaiian or Other Pacific Islander', 'race_White']\n",
    "    }\n",
    "\n",
    "\n",
    "    return data, feature_groups\n",
    "\n",
    "# Dataset creation function\n",
    "def create_pytorch_datasets(\n",
    "    data: pd.DataFrame,\n",
    "    feature_groups: Dict[str, List[str]],\n",
    "    label_col: str = 'approved',\n",
    "    test_size: float = 0.1\n",
    ") -> Dict[str, Dict[str, Dataset]]:\n",
    "    \"\"\"\n",
    "    Create PyTorch datasets for different feature groups.\n",
    "    Returns a dictionary of train-test datasets.\n",
    "    \"\"\"\n",
    "    # print(\"Creating PyTorch datasets...\")\n",
    "\n",
    "    datasets = {}\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    print(len(feature_groups))\n",
    "    for group_name, features in feature_groups.items():\n",
    "        # print(f\"Processing dataset: {group_name}\")\n",
    "\n",
    "        # Select features\n",
    "        X = data[features].copy()\n",
    "\n",
    "        # Handle categorical variables\n",
    "        # categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        # for col in categorical_cols:\n",
    "        #     X[col] = le.fit_transform(X[col])\n",
    "\n",
    "        # Create the target variable\n",
    "        y = data[label_col]\n",
    "\n",
    "        # Combine features and labels into a DataFrame\n",
    "        full_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "\n",
    "        # Create PyTorch Dataset\n",
    "        full_dataset = MortgageDataset(full_data, features, label_col)\n",
    "\n",
    "        # Split into train and test datasets\n",
    "        total_size = len(full_dataset)\n",
    "        test_size_split = int(total_size * test_size)\n",
    "        train_size_split = total_size - test_size_split\n",
    "        train_dataset, test_dataset = random_split(full_dataset, [train_size_split, test_size_split])\n",
    "\n",
    "        datasets[group_name] = {\n",
    "            'train': train_dataset,\n",
    "            'test': test_dataset\n",
    "        }\n",
    "\n",
    "    return datasets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Preprocess the data\n",
    "    data, feature_groups = preprocess_data(df)\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    datasets = create_pytorch_datasets(data, feature_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MortgageClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "        \"\"\"\n",
    "        super(MortgageClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(256, 1028),  # Hidden layer with 64 units\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(1028, 256),        # Hidden layer with 32 units\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(256, 64),        # Hidden layer with 32 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),         # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the model and record epoch times.\n",
    "    Args:\n",
    "        model (nn.Module): Neural network model\n",
    "        train_loader (DataLoader): DataLoader for training data\n",
    "        criterion (nn.Module): Loss function\n",
    "        optimizer (torch.optim.Optimizer): Optimizer\n",
    "        num_epochs (int): Number of training epochs\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    total_training_time = 0  # Total training time\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()  # Record start time for the epoch\n",
    "\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features).squeeze()\n",
    "\n",
    "            if outputs.shape != labels.shape:\n",
    "                outputs = torch.reshape(outputs, labels.shape)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Print total training time\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    Args:\n",
    "        model (nn.Module): Trained neural network model\n",
    "        test_loader (DataLoader): DataLoader for testing data\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for features, labels in test_loader:\n",
    "\n",
    "            outputs = torch.sigmoid(model(features).squeeze())\n",
    "            predictions = (outputs >= 0.5).float()  # Threshold for binary classification\n",
    "            # print('preds', predictions)\n",
    "            # print('labels', labels)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Training the model...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_loader = DataLoader(datasets['baseline']['train'], batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(datasets['baseline']['test'], batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = len(feature_groups['baseline'])\n",
    "print(input_dim)\n",
    "model = MortgageClassifier(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating the model...\")\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
