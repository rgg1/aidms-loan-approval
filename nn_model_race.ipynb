{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network model training and evaluation including race as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Load in and prepare the HMDA data for use in our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(main_file: str, sample_size: int = 500000) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Load and prepare the HMDA dataset using chunked processing and sampling.\n",
    "    Returns processed dataframe and data statistics dictionary.\n",
    "\n",
    "    Args:\n",
    "        main_file (str): The path to the main HMDA dataset file.\n",
    "        sample_size (int): The number of samples to take from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict]: Processed dataframe and data statistics dictionary.\n",
    "    \"\"\"\n",
    "    # Define only the columns we need\n",
    "    needed_columns = [\n",
    "        'action_taken',\n",
    "        'loan_amount_000s',\n",
    "        'applicant_income_000s',\n",
    "        'applicant_race_name_1',\n",
    "        'applicant_ethnicity',\n",
    "        'applicant_sex',\n",
    "        'hud_median_family_income',\n",
    "        'denial_reason_1',\n",
    "        'state_code',\n",
    "        'county_code',\n",
    "        'minority_population',\n",
    "        'tract_to_msamd_income'\n",
    "    ]\n",
    "    \n",
    "    print(\"Loading and sampling main HMDA data...\")\n",
    "    # Initialize empty list to store chunks\n",
    "    chunks = []\n",
    "    # chunk_size set depending on available memory\n",
    "    chunk_size = 100000\n",
    "\n",
    "    # Initialize the tqdm progress bar and chunking\n",
    "    for chunk in tqdm(pd.read_csv(main_file, \n",
    "                                usecols=needed_columns, \n",
    "                                chunksize=chunk_size),\n",
    "                    total=143, \n",
    "                    desc=\"Processing chunks\"):\n",
    "\n",
    "        # Sample from each chunk proportionally\n",
    "        chunk_sample_size = int(sample_size * (chunk_size / 14285496))  # Adjust for total record count\n",
    "        sampled_chunk = chunk.sample(n=min(chunk_sample_size, len(chunk)))\n",
    "        chunks.append(sampled_chunk)\n",
    "\n",
    "        # Break if we've reached the desired sample size\n",
    "        if sum(len(chunk) for chunk in chunks) >= sample_size:\n",
    "            break\n",
    "    \n",
    "    # Combine chunks\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Convert action_taken to binary (approved = 1, denied = 0)\n",
    "    df['approved'] = df['action_taken'].isin([1, 2]).astype(int)\n",
    "    \n",
    "    # Calculate approval rates by different demographics\n",
    "    stats = {}\n",
    "    \n",
    "    # Overall approval rate\n",
    "    stats['overall_approval_rate'] = df['approved'].mean()\n",
    "    \n",
    "    # Approval rates by race (using primary race)\n",
    "    race_approvals = df.groupby('applicant_race_name_1')['approved'].agg(['mean', 'count'])\n",
    "    stats['race_approval_rates'] = race_approvals\n",
    "    \n",
    "    # Approval rates by income bracket\n",
    "    df['income_bracket'] = pd.qcut(df['applicant_income_000s'].fillna(-1), \n",
    "                                 q=5, \n",
    "                                 labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "    income_approvals = df.groupby('income_bracket')['approved'].agg(['mean', 'count'])\n",
    "    stats['income_approval_rates'] = income_approvals\n",
    "    \n",
    "    # Calculate loan amount approval rates\n",
    "    # Create loan amount bins\n",
    "    df['loan_bin'] = pd.qcut(df['loan_amount_000s'].dropna(), q=10, labels=False)\n",
    "    loan_approval_rates = df.groupby('loan_bin')['approved'].mean()\n",
    "    stats['loan_amount_approval_rates'] = loan_approval_rates.tolist()\n",
    "    \n",
    "    print(\"\\nBasic dataset statistics:\")\n",
    "    print(f\"Number of applications: {len(df):,}\")\n",
    "    print(f\"Number of approved loans: {df['approved'].sum():,}\")\n",
    "    print(f\"Overall approval rate: {df['approved'].mean():.2%}\")\n",
    "    \n",
    "    return df, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Create our dataset class for the HMDA data, as well as define functions for preprocessing and further preparing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MortgageDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, features: List[str], label_col: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): Processed DataFrame. Output of preprocess_data()\n",
    "            features (List[str]): List of feature columns\n",
    "            label_col (str): Name of the target column\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(df[features].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df[label_col].values, dtype=torch.float32)\n",
    "\n",
    "        print(self.features.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "# Preprocessing function\n",
    "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Preprocess the HMDA dataset and create feature groups for analysis.\n",
    "    Returns preprocessed DataFrame and feature groups.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): HMDA dataset. Output of load_and_prepare_data()\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict[str, List[str]]]: Preprocessed DataFrame and feature groups.\n",
    "    \"\"\"\n",
    "    # Create a copy of the data\n",
    "    data = df.copy()\n",
    "\n",
    "    # Define feature groups\n",
    "    feature_groups = {\n",
    "        'baseline': [\n",
    "            'loan_amount_000s',\n",
    "            'applicant_income_000s',\n",
    "            'hud_median_family_income',\n",
    "            'tract_to_msamd_income',\n",
    "            'applicant_race_name_1',\n",
    "            # 'applicant_ethnicity'\n",
    "            # 'income_to_loan_ratio',  # Will be created\n",
    "            # 'area_income_ratio'      # Will be created\n",
    "        # ],\n",
    "        # 'location': [\n",
    "        #     'state_code',\n",
    "        #     'county_code',\n",
    "        #     'minority_population'\n",
    "        # ],\n",
    "        # 'sensitive': [\n",
    "        #     'applicant_race_name_1',\n",
    "        #     'applicant_ethnicity',\n",
    "        #     'applicant_sex'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    columns_to_keep = [\n",
    "        'loan_amount_000s',\n",
    "        'applicant_income_000s',\n",
    "        'hud_median_family_income',\n",
    "        'tract_to_msamd_income',\n",
    "        'applicant_race_name_1',\n",
    "        # 'applicant_ethnicity',\n",
    "        'approved'\n",
    "    ]   \n",
    "\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # remove entries that are missing entries\n",
    "    data = data.dropna()\n",
    "\n",
    "    data = pd.get_dummies(data, columns=['applicant_race_name_1'], prefix='race')\n",
    "\n",
    "    to_remove = ['race_Information not provided by applicant in mail, Internet, or telephone application', 'race_Not applicable']\n",
    "\n",
    "    data = data.drop(columns=to_remove)\n",
    "\n",
    "    # normalize values \n",
    "    data['loan_amount_000s'] = data['loan_amount_000s'] / data['loan_amount_000s'].mean()\n",
    "    data['applicant_income_000s'] = data['applicant_income_000s'] / data['applicant_income_000s'].mean()\n",
    "    data['hud_median_family_income'] = data['hud_median_family_income'] / data['hud_median_family_income'].mean()\n",
    "    data['tract_to_msamd_income'] = data['tract_to_msamd_income'] / data['tract_to_msamd_income'].mean()\n",
    "\n",
    "    data = data.astype({col: 'float32' for col in data.select_dtypes(include='bool').columns})\n",
    "    \n",
    "    feature_groups = {\n",
    "        'baseline': ['loan_amount_000s', 'applicant_income_000s', 'hud_median_family_income',\n",
    "       'tract_to_msamd_income',\n",
    "       'race_American Indian or Alaska Native', 'race_Asian',\n",
    "       'race_Black or African American',\n",
    "       'race_Native Hawaiian or Other Pacific Islander', 'race_White']\n",
    "    }\n",
    "\n",
    "    return data, feature_groups\n",
    "\n",
    "def create_pytorch_datasets(\n",
    "    data: pd.DataFrame,\n",
    "    feature_groups: Dict[str, List[str]],\n",
    "    label_col: str = 'approved',\n",
    "    test_size: float = 0.2\n",
    "    ) -> Dict[str, Dict[str, Dataset]]:\n",
    "    \"\"\"\n",
    "    Create PyTorch datasets for different feature groups.\n",
    "    Returns a dictionary of train-test datasets.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Preprocessed DataFrame. Output of preprocess_data()\n",
    "        feature_groups (Dict[str, List[str]]): Dictionary of feature groups. Output of preprocess_data()\n",
    "        label_col (str): Name of the target column\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dataset]]: Dictionary of train-test datasets for each feature group.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    print(len(feature_groups))\n",
    "    for group_name, features in feature_groups.items():\n",
    "        # Select features\n",
    "        X = data[features].copy()\n",
    "\n",
    "        # Create the target variable\n",
    "        y = data[label_col]\n",
    "\n",
    "        # Combine features and labels into a DataFrame\n",
    "        full_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "        # Create PyTorch Dataset\n",
    "        full_dataset = MortgageDataset(full_data, features, label_col)\n",
    "\n",
    "        # Split into train and test datasets\n",
    "        total_size = len(full_dataset)\n",
    "        test_size_split = int(total_size * test_size)\n",
    "        train_size_split = total_size - test_size_split\n",
    "        train_dataset, test_dataset = random_split(full_dataset, [train_size_split, test_size_split])\n",
    "\n",
    "        datasets[group_name] = {\n",
    "            'train': train_dataset,\n",
    "            'test': test_dataset\n",
    "        }\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Define our model, as well as the training and evaluation functions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MortgageClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "        \"\"\"\n",
    "        super(MortgageClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(256, 1028),  # Hidden layer with 64 units\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(1028, 256),        # Hidden layer with 32 units\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(256, 64),        # Hidden layer with 32 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),         # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the neural network\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the model and record epoch times.\n",
    "    Args:\n",
    "        model (nn.Module): Neural network model (instance of MortgageClassifier)\n",
    "        train_loader (DataLoader): DataLoader for training data\n",
    "        criterion (nn.Module): Loss function\n",
    "        optimizer (torch.optim.Optimizer): Optimizer\n",
    "        num_epochs (int): Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    total_training_time = 0  # Total training time\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()  # Record start time for the epoch\n",
    "\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features).squeeze()\n",
    "\n",
    "            if outputs.shape != labels.shape:\n",
    "                outputs = torch.reshape(outputs, labels.shape)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Print total training time\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    Args:\n",
    "        model (nn.Module): Trained neural network model\n",
    "        test_loader (DataLoader): DataLoader for testing data\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for features, labels in test_loader:\n",
    "\n",
    "            outputs = torch.sigmoid(model(features).squeeze())\n",
    "            predictions = (outputs >= 0.5).float()  # Threshold for binary classification\n",
    "            # print('preds', predictions)\n",
    "            # print('labels', labels)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution block to run all model training, evaluation, and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and sampling main HMDA data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  99%|█████████▉| 142/143 [00:35<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic dataset statistics:\n",
      "Number of applications: 500,500\n",
      "Number of approved loans: 271,042\n",
      "Overall approval rate: 54.15%\n",
      "1\n",
      "torch.Size([424843, 9])\n",
      "9\n",
      "Training the model...\n",
      "Epoch [1/5], Loss: 0.6579, Time: 3.26s\n",
      "Epoch [2/5], Loss: 0.6557, Time: 3.46s\n",
      "Epoch [3/5], Loss: 0.6553, Time: 3.33s\n",
      "Epoch [4/5], Loss: 0.6550, Time: 3.13s\n",
      "Epoch [5/5], Loss: 0.6548, Time: 3.03s\n",
      "Total Training Time: 16.22s\n",
      "\n",
      "Evaluating the model...\n",
      "Test Accuracy: 62.36%\n",
      "\n",
      "Evaluating the model...\n",
      "Test Accuracy: 62.36%\n",
      "\n",
      "Saving predictions and indices...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_file = 'HDMA/hmda_2017_nationwide_all-records_labels.csv'\n",
    "    \n",
    "    # Load and prepare data\n",
    "    df, stats = load_and_prepare_data(main_file)\n",
    "\n",
    "    # Preprocess the data\n",
    "    data, feature_groups = preprocess_data(df)\n",
    "\n",
    "    # Create PyTorch datasets\n",
    "    datasets = create_pytorch_datasets(data, feature_groups)\n",
    "\n",
    "    batch_size = 100\n",
    "    train_loader = DataLoader(datasets['baseline']['train'], batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(datasets['baseline']['test'], batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    input_dim = len(feature_groups['baseline'])\n",
    "    print(input_dim)\n",
    "    model = MortgageClassifier(input_dim)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training the model...\")\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    evaluate_model(model, test_loader)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    test_indices = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Get predictions and compute accuracy batch by batch\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, labels) in enumerate(test_loader):\n",
    "            # Get batch indices\n",
    "            batch_size = features.size(0)\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            test_indices.extend(range(start_idx, end_idx))\n",
    "            \n",
    "            # Get predictions for this batch\n",
    "            outputs = torch.sigmoid(model(features).squeeze())\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            predictions.extend(preds.numpy())\n",
    "            \n",
    "            # Compute accuracy for this batch\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += batch_size\n",
    "    \n",
    "    # Compute overall accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Save predictions and indices\n",
    "    print(\"\\nSaving predictions and indices...\")\n",
    "    # Check if this is the race-included model\n",
    "    if 'race' in feature_groups['baseline'][-1]:\n",
    "        np.save('nn_predictions_with_race.npy', np.array(predictions))\n",
    "    else:\n",
    "        np.save('nn_predictions_no_race.npy', np.array(predictions))\n",
    "    np.save('test_indices.npy', np.array(test_indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
