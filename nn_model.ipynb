{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network model training and evaluation without race as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict, List\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Load in and prepare the HMDA data for use in our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(main_file: str, sample_size: int = 500000) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Load and prepare the HMDA dataset using chunked processing and sampling.\n",
    "    Returns processed dataframe and data statistics dictionary.\n",
    "\n",
    "    Args:\n",
    "        main_file (str): The path to the main HMDA dataset file.\n",
    "        sample_size (int): The number of samples to take from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict]: A tuple containing the processed dataframe and data statistics dictionary.\n",
    "    \"\"\"\n",
    "    # Define only the columns we need\n",
    "    needed_columns = [\n",
    "        'action_taken',\n",
    "        'loan_amount_000s',\n",
    "        'applicant_income_000s',\n",
    "        'applicant_race_name_1',\n",
    "        'applicant_ethnicity',\n",
    "        'applicant_sex',\n",
    "        'hud_median_family_income',\n",
    "        'denial_reason_1',\n",
    "        'state_code',\n",
    "        'county_code',\n",
    "        'minority_population',\n",
    "        'tract_to_msamd_income'\n",
    "    ]\n",
    "    \n",
    "    print(\"Loading and sampling main HMDA data...\")\n",
    "    # Initialize empty list to store chunks\n",
    "    chunks = []\n",
    "    # chunk size set depending on available memory\n",
    "    chunk_size = 100000\n",
    "\n",
    "    # Initialize the tqdm progress bar and do chunking\n",
    "    for chunk in tqdm(pd.read_csv(main_file, \n",
    "                                usecols=needed_columns, \n",
    "                                chunksize=chunk_size),\n",
    "                    total=143, \n",
    "                    desc=\"Processing chunks\"):\n",
    "\n",
    "        # Sample from each chunk proportionally\n",
    "        chunk_sample_size = int(sample_size * (chunk_size / 14285496))  # Adjust for total record count\n",
    "        sampled_chunk = chunk.sample(n=min(chunk_sample_size, len(chunk)))\n",
    "        chunks.append(sampled_chunk)\n",
    "\n",
    "        # Break if we've reached the desired sample size\n",
    "        if sum(len(chunk) for chunk in chunks) >= sample_size:\n",
    "            break\n",
    "    \n",
    "    # Combine chunks\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Convert action_taken to binary (approved = 1, denied = 0)\n",
    "    df['approved'] = df['action_taken'].isin([1, 2]).astype(int)\n",
    "    \n",
    "    # Calculate approval rates by different demographics\n",
    "    stats = {}\n",
    "    \n",
    "    # Overall approval rate\n",
    "    stats['overall_approval_rate'] = df['approved'].mean()\n",
    "    \n",
    "    # Approval rates by race (using primary race)\n",
    "    race_approvals = df.groupby('applicant_race_name_1')['approved'].agg(['mean', 'count'])\n",
    "    stats['race_approval_rates'] = race_approvals\n",
    "    \n",
    "    # Approval rates by income bracket\n",
    "    df['income_bracket'] = pd.qcut(df['applicant_income_000s'].fillna(-1), \n",
    "                                 q=5, \n",
    "                                 labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "    income_approvals = df.groupby('income_bracket')['approved'].agg(['mean', 'count'])\n",
    "    stats['income_approval_rates'] = income_approvals\n",
    "    \n",
    "    # Calculate loan amount approval rates\n",
    "    # Create loan amount bins\n",
    "    df['loan_bin'] = pd.qcut(df['loan_amount_000s'].dropna(), q=10, labels=False)\n",
    "    loan_approval_rates = df.groupby('loan_bin')['approved'].mean()\n",
    "    stats['loan_amount_approval_rates'] = loan_approval_rates.tolist()\n",
    "    \n",
    "    print(\"\\nBasic dataset statistics:\")\n",
    "    print(f\"Number of applications: {len(df):,}\")\n",
    "    print(f\"Number of approved loans: {df['approved'].sum():,}\")\n",
    "    print(f\"Overall approval rate: {df['approved'].mean():.2%}\")\n",
    "    \n",
    "    return df, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Create our dataset class for the HMDA data, as well as define functions for preprocessing and further preparing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MortgageDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, features: List[str], label_col: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): Processed DataFrame. Output of preprocess_data().\n",
    "            features (List[str]): List of feature columns.\n",
    "            label_col (str): Name of the target column.\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(df[features].values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(df[label_col].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Preprocess the HMDA dataset and create feature groups for analysis.\n",
    "    Returns preprocessed DataFrame and feature groups.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The HMDA dataset. Output of load_and_prepare_data().\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Dict[str, List[str]]]: A tuple containing the preprocessed DataFrame and feature groups.\n",
    "    \"\"\"\n",
    "    # Create a copy of the data\n",
    "    data = df.copy()\n",
    "\n",
    "    # Define feature groups\n",
    "    feature_groups = {\n",
    "        'baseline': [\n",
    "            'loan_amount_000s',\n",
    "            'applicant_income_000s',\n",
    "            'hud_median_family_income',\n",
    "            'tract_to_msamd_income',\n",
    "            # 'income_to_loan_ratio',  # Will be created\n",
    "            # 'area_income_ratio'      # Will be created\n",
    "        # ],\n",
    "        # 'location': [\n",
    "        #     'state_code',\n",
    "        #     'county_code',\n",
    "        #     'minority_population'\n",
    "        # ],\n",
    "        # 'sensitive': [\n",
    "        #     'applicant_race_name_1',\n",
    "        #     'applicant_ethnicity',\n",
    "        #     'applicant_sex'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    columns_to_keep = [\n",
    "        'loan_amount_000s',\n",
    "        'applicant_income_000s',\n",
    "        'hud_median_family_income',\n",
    "        'tract_to_msamd_income',\n",
    "        'approved'\n",
    "    ]   \n",
    "\n",
    "    data = data[columns_to_keep]\n",
    "\n",
    "    # remove entries that are missing entries\n",
    "    data = data.dropna()\n",
    "\n",
    "    # normalize values \n",
    "    data['loan_amount_000s'] = data['loan_amount_000s'] / data['loan_amount_000s'].mean()\n",
    "    data['applicant_income_000s'] = data['applicant_income_000s'] / data['applicant_income_000s'].mean()\n",
    "    data['hud_median_family_income'] = data['hud_median_family_income'] / data['hud_median_family_income'].mean()\n",
    "    data['tract_to_msamd_income'] = data['tract_to_msamd_income'] / data['tract_to_msamd_income'].mean()\n",
    "    \n",
    "    return data, feature_groups\n",
    "\n",
    "def create_pytorch_datasets(\n",
    "    data: pd.DataFrame,\n",
    "    feature_groups: Dict[str, List[str]],\n",
    "    label_col: str = 'approved',\n",
    "    test_size: float = 0.2,\n",
    "    random_seed: int = 42\n",
    "    ) -> Dict[str, Dict[str, Dataset]]:\n",
    "    \"\"\"\n",
    "    Create PyTorch datasets for different feature groups.\n",
    "    Returns a dictionary of train-test datasets.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The preprocessed DataFrame. Output of preprocess_data().\n",
    "        feature_groups (Dict[str, List[str]]): Dictionary of feature groups. Output of preprocess_data().\n",
    "        label_col (str): The target column name.\n",
    "        test_size (float): The proportion of data to include in the test split.\n",
    "        random_seed (int): The random seed to use for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Dataset]]: A dictionary containing train-test datasets for each feature group\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    for group_name, features in feature_groups.items():\n",
    "        # Select features\n",
    "        X = data[features].copy()\n",
    "\n",
    "        # Create the target variable\n",
    "        y = data[label_col]\n",
    "\n",
    "        # Combine features and labels into a DataFrame\n",
    "        full_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "        # Create PyTorch Dataset\n",
    "        full_dataset = MortgageDataset(full_data, features, label_col)\n",
    "\n",
    "        # Split into train and test datasets\n",
    "        total_size = len(full_dataset)\n",
    "        test_size_split = int(total_size * test_size)\n",
    "        train_size_split = total_size - test_size_split\n",
    "        train_dataset, test_dataset = random_split(full_dataset, [train_size_split, test_size_split], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "        datasets[group_name] = {\n",
    "            'train': train_dataset,\n",
    "            'test': test_dataset\n",
    "        }\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Define our model, as well as the training and evaluation functions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MortgageClassifier(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of input features\n",
    "        \"\"\"\n",
    "        super(MortgageClassifier, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 64),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),  # Hidden layer with 64 units\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(256, 1028),  # Hidden layer with 64 units\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(1028, 256),        # Hidden layer with 32 units\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(256, 64),        # Hidden layer with 32 units\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),         # Output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the model and record epoch times.\n",
    "    Args:\n",
    "        model (nn.Module): Neural network model (instance of MortgageClassifier)\n",
    "        train_loader (DataLoader): DataLoader for training data\n",
    "        criterion (nn.Module): Loss function\n",
    "        optimizer (torch.optim.Optimizer): Optimizer\n",
    "        num_epochs (int): Number of training epochs\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    total_training_time = 0  # Total training time\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()  # Record start time for the epoch\n",
    "\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features).squeeze()\n",
    "\n",
    "            if outputs.shape != labels.shape:\n",
    "                outputs = torch.reshape(outputs, labels.shape)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    # Print total training time\n",
    "    print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    Args:\n",
    "        model (nn.Module): Trained neural network model\n",
    "        test_loader (DataLoader): DataLoader for testing data\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for features, labels in test_loader:\n",
    "\n",
    "            outputs = torch.sigmoid(model(features).squeeze())\n",
    "            predictions = (outputs >= 0.5).float()  # Threshold for binary classification\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution block to run all model training, evaluation, and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and sampling main HMDA data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  99%|█████████▉| 142/143 [00:40<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic dataset statistics:\n",
      "Number of applications: 500,500\n",
      "Number of approved loans: 271,149\n",
      "Overall approval rate: 54.18%\n",
      "4\n",
      "Training the model...\n",
      "Epoch [1/5], Loss: 0.6657, Time: 3.62s\n",
      "Epoch [2/5], Loss: 0.6641, Time: 3.25s\n",
      "Epoch [3/5], Loss: 0.6633, Time: 3.50s\n",
      "Epoch [4/5], Loss: 0.6631, Time: 3.37s\n",
      "Epoch [5/5], Loss: 0.6630, Time: 3.56s\n",
      "Total Training Time: 17.31s\n",
      "\n",
      "Evaluating the model...\n",
      "Test Accuracy: 61.34%\n",
      "\n",
      "Saving predictions and labels...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    RANDOM_SEED = 42\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    main_file = 'HDMA/hmda_2017_nationwide_all-records_labels.csv'\n",
    "    # Load and prepare data\n",
    "    df, stats = load_and_prepare_data(main_file)\n",
    "    # Preprocess the data\n",
    "    data, feature_groups = preprocess_data(df)\n",
    "    # Create PyTorch datasets\n",
    "    datasets = create_pytorch_datasets(data, feature_groups, test_size=0.2, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    batch_size = 100\n",
    "    test_dataset = datasets['baseline']['test']\n",
    "    test_indices = test_dataset.indices\n",
    "    \n",
    "    # Important: Create test loader without dropping last batch\n",
    "    train_loader = DataLoader(datasets['baseline']['train'], batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(datasets['baseline']['test'], batch_size=batch_size, shuffle=False, drop_last=False)  # Changed these parameters\n",
    "    \n",
    "    # Rest of the initialization code...\n",
    "    input_dim = len(feature_groups['baseline'])\n",
    "    print(input_dim)\n",
    "    model = MortgageClassifier(input_dim)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    used_labels = []  # Keep track of the actual labels we use\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Get predictions batch by batch\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            outputs = torch.sigmoid(model(features).squeeze())\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            predictions.extend(preds.numpy())\n",
    "            used_labels.extend(labels.numpy())  # Save the actual labels we used\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += len(labels)  # Use actual batch size instead of fixed size\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    used_labels = np.array(used_labels)\n",
    "    \n",
    "    # Compute overall accuracy\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Save predictions and the actual labels used\n",
    "    print(\"\\nSaving predictions and labels...\")\n",
    "    if 'race' in feature_groups['baseline'][-1]:\n",
    "        np.save('nn_predictions_with_race.npy', predictions)\n",
    "        np.save('nn_test_labels_with_race.npy', used_labels)\n",
    "    else:\n",
    "        np.save('nn_predictions_no_race.npy', predictions)\n",
    "        np.save('nn_test_labels_no_race.npy', used_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
